<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academia 4.3.1"><meta name=generator content="Hugo 0.104.3"><meta name=author content="Hila Chefer"><meta name=description content><link rel=alternate hreflang=en-us href=https://examplesite.org/><meta name=theme-color content="#b54845"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap"><link rel=stylesheet href=/css/academia.min.d1d8d530dfa7b55780d5887faa05b19b.css><link rel=alternate href=/index.xml type=application/rss+xml title="Hila Chefer"><link rel=feed href=/index.xml type=application/rss+xml title="Hila Chefer"><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/icon.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://examplesite.org/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@hila_chefer"><meta property="twitter:creator" content="@hila_chefer"><meta property="og:site_name" content="Hila Chefer"><meta property="og:url" content="https://examplesite.org/"><meta property="og:title" content="Hila Chefer"><meta property="og:description" content><meta property="og:image" content="https://examplesite.org/img/icon-192.png"><meta property="twitter:image" content="https://examplesite.org/img/icon-192.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-11-20T00:00:00+00:00"><title>Hila Chefer</title></head><body id=top data-spy=scroll data-target=#navbar-main data-offset=71><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id=navbar-main><div class=container><a class=navbar-brand href=/>Hila Chefer</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#publications data-target=#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#talks data-target=#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#posts data-target=#posts><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/#contact data-target=#contact><span>Contact</span></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=container><div class=row itemprop=author itemscope itemtype=http://schema.org/Person itemref="person-email person-address"><div class=col-lg-4><div id=profile><img class="img-fluid hero-img" src=/img/user-2.jpg itemprop=image alt=Avatar></div><ul class="network-icon bg-white mx-2 py-2 text-center" aria-hidden=true><li class=mx-2><a itemprop=sameAs href=https://github.com/hila-chefer target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li class=mx-2><a itemprop=sameAs href="https://scholar.google.com/citations?user=B8sA9JoAAAAJ&hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li class=mx-2><a itemprop=sameAs href=https://www.semanticscholar.org/author/Hila-Chefer/2038268012 target=_blank rel=noopener><i class="ai ai-semantic-scholar"></i></a></li><li class=mx-2><a itemprop=sameAs href=https://twitter.com/hila_chefer target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li class=mx-2><a itemprop=sameAs href=https://www.linkedin.com/in/hila-chefer target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div><div class=col-lg-8 itemprop=description><div class=portrait-title><h2 itemprop=name>Hila Chefer</h2></div><link itemprop=url href><p>I am a Ph.D. Candidate at Tel Aviv University, working in the Deep Learning Lab under the supervision of <a href=http://www.cs.tau.ac.il/~wolf/>Prof. Lior Wolf.</a></p><p>My main interests are computer vision, NLP, and multi-modal learning.
My research is mostly focused on attention-based models and explainable-AI.</p></div></div></div></section><section id=publications class="home-section wg-pages"><div class=container><div class=row><div class="col-12 section-heading text-center"><h1>Publications</h1></div><div class=col-12><div class="row justify-content-center"><div class="col-lg-4 col-md-6 mb-4"><div class=card-simple itemscope itemtype=http://schema.org/ScholarlyArticle><a href=/publication/optimizing_relevance_for_robustness/><img src=/publication/optimizing_relevance_for_robustness/featured_hu96d1fd6528c4aa99ac1ab603bc2aadc7_2835564_918x517_fill_q90_lanczos_smart1_3.png class=article-banner itemprop=image alt></a><div class=card-body><h3 class="article-title mb-1 mt-0" itemprop=name><a href=/publication/optimizing_relevance_for_robustness/ itemprop=url>Optimizing Relevance Maps of Vision Transformers Improves Robustness</a></h3><meta content="2022-11-20 00:00:00 +0000 UTC" itemprop=datePublished><meta content="2022-11-20 00:00:00 +0000 UTC" itemprop=dateModified><div class=article-metadata><div><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/idan-schwartz/>Idan Schwartz</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/lior-wolf/>Lior Wolf</a></span></div><span class=article-date><time>November 2022</time></span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>NeurIPS</em></span></div><div class=article-style itemprop=articleBody>Vision models are known to use &ldquo;shortcuts&rdquo; in the data, i.e. use irrelevant cues, such as the image background, to achieve high accuracy. In this work, we show that using a very short and simple <em>few-shot</em> finetuning process on the relevance maps of a Vision Transformer, we can teach the model <em>why</em> the label is correct, and enforce that the predictions are based on the <em>right</em> reasons. We demonstrate a significant improvement in the robustness of the Vision Transformers (ViTs) to distribution shifts.</div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/abs/2206.01161 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/optimizing_relevance_for_robustness/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/hila-chefer/RobustViT target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://huggingface.co/spaces/Hila/RobustViT target=_blank rel=noopener>Gradio demo</a></div></div></div></div><div class="col-lg-4 col-md-6 mb-4"><div class=card-simple itemscope itemtype=http://schema.org/ScholarlyArticle><a href=/publication/clip-guided-essence-transfer/><img src=/publication/clip-guided-essence-transfer/featured_hu3ba55b2e456e9f6d03500c3b6488890b_1132122_918x517_fill_q90_lanczos_smart1_3.png class=article-banner itemprop=image alt></a><div class=card-body><h3 class="article-title mb-1 mt-0" itemprop=name><a href=/publication/clip-guided-essence-transfer/ itemprop=url>Image-Based Clip-Guided Essence Transfer</a></h3><meta content="2022-10-20 00:00:00 +0000 UTC" itemprop=datePublished><meta content="2022-10-20 00:00:00 +0000 UTC" itemprop=dateModified><div class=article-metadata><div><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/sagie-benaim/>Sagie Benaim</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/roni-paiss/>Roni Paiss</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/lior-wolf/>Lior Wolf</a></span></div><span class=article-date><time>October 2022</time></span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>ECCV</em></span></div><div class=article-style itemprop=articleBody>This paper proposes a novel method to transfer the semantic properties that constitute high-level textual description from a target image to a source image, without changing the identity of the source. The method uses CLIP&rsquo;s image latent space, which is more stable and expressive than the textual latent space.</div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/abs/2110.12427 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/clip-guided-essence-transfer/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/hila-chefer/TargetCLIP target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://www.casualganpapers.com/clip_image_to_image_style_transfer_essence_transfer/TargetCLIP-explained.html target=_blank rel=noopener>5 min summary</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.youtube.com/watch?v=GY_g4aDi_ys" target=_blank rel=noopener>5 min video (ECCV)</a></div></div></div></div><div class="col-lg-4 col-md-6 mb-4"><div class=card-simple itemscope itemtype=http://schema.org/ScholarlyArticle><a href=/publication/no-token-left-behind/><img src=/publication/no-token-left-behind/featured_huca748f77febde25ffe8db2fd73bd237e_216745_918x517_fill_q90_lanczos_smart1_3.png class=article-banner itemprop=image alt></a><div class=card-body><h3 class="article-title mb-1 mt-0" itemprop=name><a href=/publication/no-token-left-behind/ itemprop=url>No Token Left Behind: Explainability-Aided Image Classification and Generation</a></h3><meta content="2022-10-11 00:00:00 +0000 UTC" itemprop=datePublished><meta content="2022-10-11 00:00:00 +0000 UTC" itemprop=dateModified><div class=article-metadata><div><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/roni-paiss/>Roni Paiss</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/lior-wolf/>Lior Wolf</a></span></div><span class=article-date><time>October 2022</time></span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>ECCV</em></span></div><div class=article-style itemprop=articleBody>The paper presents a novel use of explainability to perform zero-shot tasks such as image classification and generation. We demonstrate that CLIP guidance based on pure similarity scores between the image and text is unstable as the scores can be based on irrelevant or partial data. Our method demonstrates the effectiveness of using explainability to stabilize the scores.</div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/abs/2204.04908 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/no-token-left-behind/cite.bib>
Cite</button></div></div></div></div><div class="col-lg-4 col-md-6 mb-4"><div class=card-simple itemscope itemtype=http://schema.org/ScholarlyArticle><a href=/publication/generic-attention-model-explainability/><img src=/publication/generic-attention-model-explainability/featured_huc6aefd30d1d750f118e14dfc8a62151a_288878_918x517_fill_q90_lanczos_smart1_3.png class=article-banner itemprop=image alt></a><div class=card-body><h3 class="article-title mb-1 mt-0" itemprop=name><a href=/publication/generic-attention-model-explainability/ itemprop=url>Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers (Oral)</a></h3><meta content="2021-10-01 00:00:00 +0000 UTC" itemprop=datePublished><meta content="2021-10-01 00:00:00 +0000 UTC" itemprop=dateModified><div class=article-metadata><div><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/shir-gur/>Shir Gur</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/lior-wolf/>Lior Wolf</a></span></div><span class=article-date><time>October 2021</time></span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>ICCV</em></span></div><div class=article-style itemprop=articleBody>The paper presents an interpretability method for all types of attention, including bi-modal Transformers and encoder-decoder Transformers. The method achieves SOTA results for CLIP, DETR, LXMERT, and more.</div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://openaccess.thecvf.com/content/ICCV2021/papers/Chefer_Generic_Attention-Model_Explainability_for_Interpreting_Bi-Modal_and_Encoder-Decoder_Transformers_ICCV_2021_paper.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/generic-attention-model-explainability/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/hila-chefer/Transformer-MM-Explainability target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.youtube.com/watch?v=bQTL34Dln-M&t=236s" target=_blank rel=noopener>12 min summary (ICCV)</a></div></div></div></div><div class="col-lg-4 col-md-6 mb-4"><div class=card-simple itemscope itemtype=http://schema.org/ScholarlyArticle><a href=/publication/transformer-interpretability/><img src=/publication/transformer-interpretability/featured_hu802f1524ea8eb9f338b74ff95afb04c0_199607_918x517_fill_q90_lanczos_smart1_3.png class=article-banner itemprop=image alt></a><div class=card-body><h3 class="article-title mb-1 mt-0" itemprop=name><a href=/publication/transformer-interpretability/ itemprop=url>Transformer Interpretability Beyond Attention Visualization</a></h3><meta content="2021-06-01 00:00:00 +0000 UTC" itemprop=datePublished><meta content="2021-06-01 00:00:00 +0000 UTC" itemprop=dateModified><div class=article-metadata><div><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/shir-gur/>Shir Gur</a></span>, <span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/lior-wolf/>Lior Wolf</a></span></div><span class=article-date><time>June 2021</time></span>
<span class=middot-divider></span>
<span class=pub-publication>In <em>CVPR</em></span></div><div class=article-style itemprop=articleBody>This paper presents an interpretability method for self-attention based models, and specifically for Transformer encoders. The method incorporates LRP and gradients, and achieves SOTA results for ViT, BERT, and DeiT.</div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/transformer-interpretability/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/hila-chefer/Transformer-Explainability target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://analyticsindiamag.com/compute-relevancy-of-transformer-networks-via-novel-interpretable-transformer/ target=_blank rel=noopener>AIM Article</a></div></div></div></div></div></div></div></div></section><section id=talks class="home-section wg-pages"><div class=container><div class=row><div class="col-12 section-heading text-center"><h1>Recent & Upcoming Talks</h1></div><div class=col-12><div class="row justify-content-center"><div class="col-md-10 col-lg-8"><div class="media stream-item shadow p-4" itemscope itemtype=http://schema.org/Event><div class=media-body><h3 class="article-title mb-0 mt-0" itemprop=name><a href=/talk/columbia-08-2022/ itemprop=url>Transformer Explainability Beyond Accountability (English)</a></h3><div class=article-style itemprop=articleBody>This talk takes a deep dive into Transformer explainability algorithms, and demonstrates how explainability can be used to improve downstream tasks such as image editing, and even increase robustness and accuracy of image backbones.</div><div class="stream-meta article-metadata"><div><span itemprop=startDate>Aug 4, 2022
11:00
&mdash; 12:00</span>
<span class=middot-divider></span>
<span itemprop=location>Columbia university</span></div><div itemprop=author><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://m.youtube.com/watch?v=A1tqsEkSoLg" target=_blank rel=noopener>Video</a></div></div><div class=ml-3></div></div></div><div class="col-md-10 col-lg-8"><div class="media stream-item shadow p-4" itemscope itemtype=http://schema.org/Event><div class=media-body><h3 class="article-title mb-0 mt-0" itemprop=name><a href=/talk/microsoft-01-2022/ itemprop=url>Intro to Transformers and Transformer Explainability (English)</a></h3><div class=article-style itemprop=articleBody>This talk takes a deep dive into the attention mechanism. During the talk, we review the motivations and applications of the self-attention mechanism. Additionally, we review the main building blocks for self-attention explainability and some cool applications of Transformer-explainability from recent research.</div><div class="stream-meta article-metadata"><div><span itemprop=startDate>Jan 4, 2022</span>
<span class=middot-divider></span>
<span itemprop=location>Online event</span></div><div itemprop=author><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://youtu.be/a0O_QhE9XFM target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=/talk/microsoft-01-2022/><img src=/talk/microsoft-01-2022/featured_hu6a6c9513e1447a4ee2efbfa369de6f67_164569_250x0_resize_lanczos_3.png itemprop=image></a></div></div></div><div class="col-md-10 col-lg-8"><div class="media stream-item shadow p-4" itemscope itemtype=http://schema.org/Event><div class=media-body><h3 class="article-title mb-0 mt-0" itemprop=name><a href=/talk/imvc-10-2021/ itemprop=url>Transformer Explainability (English)</a></h3><div class=article-style itemprop=articleBody>This talk explores the main milestones in Transformer-based research, and Transformer explainability research.</div><div class="stream-meta article-metadata"><div><span itemprop=startDate>Oct 26, 2021</span>
<span class=middot-divider></span>
<span itemprop=location>Tel Aviv, Israel</span></div><div itemprop=author><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://youtu.be/tE2zXsVw0uo target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=/talk/imvc-10-2021/><img src=/talk/imvc-10-2021/featured_hu31871538f3ea3117c4876792da5634a5_194084_250x0_resize_lanczos_3.png itemprop=image></a></div></div></div><div class="col-md-10 col-lg-8"><div class="media stream-item shadow p-4" itemscope itemtype=http://schema.org/Event><div class=media-body><h3 class="article-title mb-0 mt-0" itemprop=name><a href=/talk/ibm-10-2021/ itemprop=url>Transformer Explainability (Hebrew)</a></h3><div class=article-style itemprop=articleBody>This talk explores the main milestones in Transformer-based research, and Transformer explainability research.</div><div class="stream-meta article-metadata"><div><span itemprop=startDate>Oct 20, 2021</span>
<span class=middot-divider></span>
<span itemprop=location>Online event</span></div><div itemprop=author><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://m.box.com/shared_item/https%3A%2F%2Fibm.box.com%2Fs%2Ffvueykzqs4pi59u55fks7ugx3p4g8ba4 target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=/talk/ibm-10-2021/><img src=/talk/ibm-10-2021/featured_hu05b1e7b25731dcb3c39e7dc7ff3febb8_530600_250x0_resize_lanczos_3.png itemprop=image></a></div></div></div><div class="col-md-10 col-lg-8"><div class="media stream-item shadow p-4" itemscope itemtype=http://schema.org/Event><div class=media-body><h3 class="article-title mb-0 mt-0" itemprop=name><a href=/talk/w-ai-07-2021/ itemprop=url>Attention in Deep Learning (Hebrew)</a></h3><div class=article-style itemprop=articleBody>This talk is an intro talk to DNNs and attention, targeted at DL beginners. The talk was given as part of a volunteering program to encourage women to consider research in the deep learning field.</div><div class="stream-meta article-metadata"><div><span itemprop=startDate>Jul 11, 2021</span>
<span class=middot-divider></span>
<span itemprop=location>Tel Aviv, Israel</span></div><div itemprop=author><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/hila-chefer/>Hila Chefer</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.youtube.com/watch?v=pseZwRyLrtc" target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=/talk/w-ai-07-2021/><img src=/talk/w-ai-07-2021/featured_hua5821d53e8e81ca916f9bd6fba183a68_172171_250x0_resize_lanczos_3.png itemprop=image></a></div></div></div></div></div></div></div></section><section id=posts class="home-section wg-pages"><div class=container><div class=row><div class="col-12 section-heading text-center"><h1>Blog</h1><p>COMING SOON!</p></div><div class=col-12><div class="row justify-content-center"></div></div></div></div></section><section id=contact class="home-section wg-contact"><div class=container><div class="row contact-widget"><div class="col-12 text-center section-heading"><h1>Contact</h1></div><div class=col-12></div><div class=col-12><div class="d-flex justify-content-center"><ul class=fa-ul itemscope><li><i class="fa-li fas fa-envelope mt-2" aria-hidden=true></i>
<span id=person-email itemprop=email>hilach70 at gmail dot com</span></li><li><i class="fa-li fas fa-map-marker-alt mt-2" aria-hidden=true></i>
<span id=person-address itemprop=address>Tel-Aviv University, Israel</span></li></ul></div></div><div class=col-12></div></div></div></section><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script>
<script>hljs.initHighlightingOnLoad()</script><script src=/js/academia.min.6c2ba2801d406881b3c2277043cedd76.js></script><div class=container><footer class=site-footer><div class=container><div class="row align-items-center"><div class="col-md-6 mb-4 mb-md-0"><p class=mb-0>Copyright Â© 2022 &#183;
Powered by
<a href=https://gethugothemes.com target=_blank rel=noopener>Gethugothemes</a></p></div><div class=col-md-6><ul class="list-inline network-icon text-right mb-0"><li class=list-inline-item><a href=https://github.com/hila-chefer target=_blank rel=noopener title=Github><i class="fab fa-github" aria-hidden=true></i></a></li><li class=list-inline-item><a href="https://scholar.google.com/citations?user=B8sA9JoAAAAJ&hl=en" target=_blank rel=noopener title="Google Scholar"><i class="ai ai-google-scholar" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://www.semanticscholar.org/author/Hila-Chefer/2038268012 target=_blank rel=noopener title="Semantic Scholar"><i class="ai ai-semantic-scholar" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://twitter.com/hila_chefer target=_blank rel=noopener title=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=list-inline-item><a href=https://www.linkedin.com/in/hila-chefer target=_blank rel=noopener title=LinkedIn><i class="fab fa-linkedin" aria-hidden=true></i></a></li></ul></div></div></div></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div></body></html>